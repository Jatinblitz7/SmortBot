{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b595ea2-d951-4071-aec3-e98cd3e10c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\BotProj\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error fetching or processing , exception: Exactly one of file, filename and url must be specified.\n",
      "Error fetching or processing , exception: Exactly one of file, filename and url must be specified.\n",
      "C:\\Users\\Jatin\\BotProj\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jatin\\BotProj\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jatin\\BotProj\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jatin\\BotProj\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jatin\\BotProj\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jatin\\BotProj\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gradio as gr\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "FAISS_FILE = \"faiss_store.pkl\"\n",
    "\n",
    "# Load model pipeline\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"MBZUAI/LaMini-Flan-T5-783M\",\n",
    "    device=device,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    temperature=0\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Prompt template\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant. Answer the question strictly using the context. If the answer is not about the exact car model mentioned in the question, say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Function with progress bar\n",
    "def process_urls(url1, url2, url3, progress=gr.Progress()):\n",
    "    urls = [url1, url2, url3]\n",
    "    loader = UnstructuredURLLoader(urls=urls)\n",
    "\n",
    "    progress(0.1, desc=\"üì• Loading articles...\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    def clean_text(text):\n",
    "        import re\n",
    "        return re.sub(r\"(Remove Ad|Story continues below.*?|Reuters|Advertisement)\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    for doc in docs:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "\n",
    "    progress(0.4, desc=\"üîç Splitting documents...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=80)\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    progress(0.6, desc=\"üìê Generating embeddings...\")\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(splits, embedding)\n",
    "\n",
    "    progress(0.9, desc=\"üíæ Saving vector store...\")\n",
    "    with open(FAISS_FILE, \"wb\") as f:\n",
    "        pickle.dump(vectorstore, f)\n",
    "\n",
    "    progress(1.0, desc=\"‚úÖ Done!\")\n",
    "    return \"‚úÖ URLs processed and saved successfully!\"\n",
    "\n",
    "# QA function\n",
    "def answer_query(question):\n",
    "    if not os.path.exists(FAISS_FILE):\n",
    "        return \"‚ùå Please process URLs first.\", \"\"\n",
    "\n",
    "    with open(FAISS_FILE, \"rb\") as f:\n",
    "        vectorstore = pickle.load(f)\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", k=3)\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": custom_prompt}\n",
    "    )\n",
    "\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    answer = result[\"result\"]\n",
    "    sources = \"\\n\".join(set(doc.metadata[\"source\"] for doc in result[\"source_documents\"]))\n",
    "    return answer, sources\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"blue\")) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "        <div style='\n",
    "        text-align: center;\n",
    "        padding: 2rem 1rem;\n",
    "        background: linear-gradient(135deg, #eaf4ff, #fdfdff);\n",
    "        border-radius: 16px;\n",
    "        margin-bottom: 20px;\n",
    "        box-shadow: 0 8px 16px rgba(0,0,0,0.05);\n",
    "        '>\n",
    "        <h1 style='\n",
    "            font-size: 2.5rem;\n",
    "            margin-bottom: 0.5rem;\n",
    "            color: #1f3b4d;\n",
    "        '>üìö SmortBot</h1>\n",
    "        \n",
    "        <p style='\n",
    "            font-size: 1.1rem;\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "            color: #333;\n",
    "        '>\n",
    "            Instantly understand news articles by asking questions. Paste URLs and ask away ‚Äî your AI analyst has got you covered!\n",
    "        </p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "    with gr.Tab(\"üåê Step 1: Process Article URLs\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                url1 = gr.Textbox(label=\"News URL 1\", placeholder=\"Paste article link...\", lines=1)\n",
    "                url2 = gr.Textbox(label=\"News URL 2\", placeholder=\"Paste article link...\", lines=1)\n",
    "                url3 = gr.Textbox(label=\"News URL 3\", placeholder=\"Paste article link...\", lines=1)\n",
    "                process_btn = gr.Button(\"üöÄ Process Articles\")\n",
    "                process_status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "        process_btn.click(process_urls, inputs=[url1, url2, url3], outputs=process_status)\n",
    "\n",
    "    with gr.Tab(\"ü§ñ Step 2: Ask Your Question\"):\n",
    "        with gr.Row():\n",
    "            question = gr.Textbox(label=\"Ask something from the articles\", placeholder=\"Any question related to the article\", lines=2)\n",
    "        with gr.Row():\n",
    "            answer = gr.Textbox(label=\"SmortBot's Answer\", lines=4, interactive=False)\n",
    "        with gr.Row():\n",
    "            sources = gr.Textbox(label=\"Sources\", lines=3, interactive=False)\n",
    "        ask_btn = gr.Button(\"üí¨ Get Answer\")\n",
    "        ask_btn.click(answer_query, inputs=question, outputs=[answer, sources])\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
